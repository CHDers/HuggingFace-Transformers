{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linking: https://blog.csdn.net/weixin_44376341/article/details/119956299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用torchtext处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入常用库\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "# 比较新版本的需要使用torchtext.legacy.data，旧版本的torchtext使用torchtex.data\n",
    "from torchtext.data import TabularDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>You or me?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>You guys are messing with me, right?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>Yeah.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>That was a good one. For a second there, I was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              utterance  label\n",
       "0     also I was the point person on my company’s tr...      0\n",
       "1                      You must’ve had your hands full.      0\n",
       "2                               That I did. That I did.      0\n",
       "3         So let’s talk a little bit about your duties.      0\n",
       "4                                My duties?  All right.      1\n",
       "...                                                 ...    ...\n",
       "9984                                         You or me?      0\n",
       "9985  I got it. Uh, Joey, women don't have Adam's ap...      0\n",
       "9986               You guys are messing with me, right?      1\n",
       "9987                                              Yeah.      0\n",
       "9988  That was a good one. For a second there, I was...      1\n",
       "\n",
       "[9989 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入数据集\n",
    "train_data = pd.read_csv('train_data_sentiment.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Field\n",
    "# 这里使用默认分词器split(),按照空格进行分词\n",
    "TEXT = torchtext.data.Field(sequential=True, lower=True, fix_length=30)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = TabularDataset(path='train_data_sentiment.csv',\n",
    "                         format='csv', skip_header=True,\n",
    "                         fields=[('utterance', TEXT), ('label', LABEL)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建词表\n",
    "TEXT.build_vocab(train_x)  #构建了10440个词，从0-10439\n",
    "for w,i in TEXT.vocab.stoi.items():\n",
    "    print(w,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载glove词向量，第一次使用会自动下载，也可以自己下载好该词向量，我这里用的是400000词，每个词由100维向量表示\n",
    "TEXT.vocab.load_vectors('glove.6B.100d',unk_init=torch.Tensor.normal_) #将数据中有但glove词向量中不存在的词进行随机初始化分配100维向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.vectors.shape) #torch.Size([10440, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建迭代器\n",
    "batch_size = 64\n",
    "train_iter = torchtext.data.Iterator(\n",
    "    dataset=train_x, batch_size=64, shuffle=True, sort_within_batch=False, repeat=False, device=device)\n",
    "len(train_iter)  # 157\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看构建的迭代器\n",
    "list(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看批数据的大小\n",
    "for batch in train_iter:\n",
    "    print(batch.utterance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看第一条数据\n",
    "batch.utterance[:,0]#我们取的是第1列，因为第1列表示第一条数据，即第64列表示第64条数据。每条数据由30个词组成，下面非1部分表示第一条数据中的词在词表中的索引，剩下的1表示补长的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看第一条数据中的词所对应的索引值\n",
    "list_a=[]\n",
    "for i in batch.utterance[:,0]:\n",
    "    if i.item()!=1:\n",
    "        list_a.append(i.item())\n",
    "print(list_a)\n",
    "for i in list_a:\n",
    "    print(TEXT.vocab.itos[i],end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看迭代器中的数据及其对应的文本\n",
    "l =[]\n",
    "for batch in list(train_iter)[:1]:\n",
    "    for i in batch.utterance:\n",
    "        l.append(i[0].item())\n",
    "    print(l)\n",
    "    print(' '.join([TEXT.vocab.itos[i] for  i in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建LSTM+Self-Attention网络模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab_size: 构建的词表中的词数\n",
    "- embedding_size: 每个词的词向量维度\n",
    "- hidden_dim：LSTM中隐藏层的单元个数\n",
    "- n_layers：LSTM中的隐藏层数量\n",
    "- num_class：类别数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10440\n",
    "embedding_size = 100\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "num_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, num_class):\n",
    "        super(LSTM_Attention, self).__init__()\n",
    "\n",
    "        # 从LSTM得到output之后，将output通过下面的linear层，然后就得到了Q,K,V\n",
    "        # 这里我是用的attention_size是等于hidden_dim的，这里可以自己换成别的attention_size\n",
    "        self.W_Q = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        # embedding层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM\n",
    "        self.rnn = nn.LSTM(input_size=embedding_dim,\n",
    "                           hidden_size=hidden_dim, num_layers=n_layers)\n",
    "        # Linear层,因为是三分类，所以后面的维度为3\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    # 用来计算attention\n",
    "    def attention(self, Q, K, V):\n",
    "\n",
    "        d_k = K.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(1, 2)) / math.sqrt(d_k)\n",
    "        alpha_n = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(alpha_n, V)\n",
    "\n",
    "        # 这里都是组合之后的矩阵之间的计算，所以.sum之后，得到的output维度就是[batch_size,hidden_dim]，并且每一行向量就表示一句话，所以总共会有batch_size行\n",
    "        output = context.sum(1)\n",
    "\n",
    "        return output, alpha_n\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x.shape = [seq_len,batch_size] = [30,64]\n",
    "\n",
    "        # embedding.shape = [seq_len,batch_size,embedding_dim = 100]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding.shape = [batch_size,seq_len,embedding_dim]\n",
    "        embedding = embedding.transpose(0, 1)\n",
    "        # 进行LSTM\n",
    "        # out.shape = [batch_size,seq_len,hidden_dim=128]\n",
    "        output, (h_n, c) = self.rnn(embedding)\n",
    "\n",
    "        Q = self.W_Q(output)  # [batch_size,seq_len,hidden_dim]\n",
    "        K = self.W_K(output)\n",
    "        V = self.W_V(output)\n",
    "\n",
    "        # 将得到的Q，K，V送入attention函数进行运算\n",
    "        attn_output, alpha_n = self.attention(Q, K, V)\n",
    "        # attn_output.shape = [batch_size,hidden_dim=128]\n",
    "        #alpha_n.shape = [batch_size,seq_len,seq_len]\n",
    "\n",
    "        out = self.fc(attn_output)  # out.shape = [batch_size,num_class]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_Attention(\n",
       "  (W_Q): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (W_K): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (W_V): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (embedding): Embedding(10440, 100)\n",
       "  (rnn): LSTM(100, 128)\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看一下我们搭建的网络模型\n",
    "net = LSTM_Attention(vocab_size=vocab_size, embedding_dim=embedding_size,\n",
    "                     hidden_dim=hidden_dim, n_layers=n_layers, num_class=num_class).to(device)\n",
    "net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练及结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding.weight.data.copy_(TEXT.vocab.vectors)  # 给模型的Embedding层传入我们的词嵌入矩阵\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)  # 定义优化器，lr是学习率可以自己调\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # 定义损失函数\n",
    "train_x_len = len(train_x)  # 这一步是我为了计算后面的Acc而获取的数据数量，也就是9989\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(net, iterator, optimizer, criterion, train_x_len):\n",
    "    epoch_loss = 0  # 初始化loss值\n",
    "    epoch_acc = 0  # 初始化acc值\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        preds = net(batch.utterance)  # 前向传播，求出预测值\n",
    "        loss = criterion(preds, batch.label)  # 计算loss\n",
    "        epoch_loss += loss.item()  # 累加loss，作为下面求平均loss的分子\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新网络中的权重参数\n",
    "        epoch_acc += ((preds.argmax(axis=1)) ==\n",
    "                      batch.label).sum().item()  # 累加acc，作为下面求平均acc的分子\n",
    "    return epoch_loss/(len(iterator)), epoch_acc/train_x_len  # 返回的是loss值和acc值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "acc_plot = []  # 用于后面画图\n",
    "loss_plot = []  # 用于后面画图\n",
    "for epoch in range(n_epoch):\n",
    "    train_loss, train_acc = train(\n",
    "        net, train_iter, optimizer, criterion, train_x_len)\n",
    "    acc_plot.append(train_acc)\n",
    "    loss_plot.append(train_loss)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('epoch: %d \\t loss: %.4f \\t train_acc: %.4f' %\n",
    "              (epoch+1, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用画图函数matplotlib\n",
    "plt.figure(figsize=(10, 5), dpi=80)\n",
    "plt.plot(acc_plot, label='train_acc')\n",
    "plt.plot(loss_plot, color='coral', label='train_loss')\n",
    "plt.legend(loc=0)\n",
    "plt.grid(True, linestyle='--', alpha=1)\n",
    "plt.xlabel('epoch', fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
